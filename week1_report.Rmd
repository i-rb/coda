---
title: "Week 1 report"
output:
  html_notebook:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    toc: yes
---

# 1. Introduction to Compositional Data

In this notebook I am computing the functions that are mentioned in the _Week 1 Report_. When possible, I compare my results with those obtained via the 'compositions' package.

```{r}
library(compositions)
```


## 1.1. First Definitions
### Function 1. Which of these variables are compositional?

This function takes a set of variables in a list (each of them matrices or vectors) and returns a vector of zeros and ones. The component in position _i_, associated to the _i_ position of the input vector, should be one if that matrix add up to a constant value and zero otherwise. This could be adjusted to add up approximately to a constant because of the roundings in transformed variables and so.


```{r} 
which_coda=function(l,eps){ # l = list of variables, # eps = allowed error
  result = numeric(length(l))
  cont=1
  for (i in l){
    if (is.integer(dim(i)[2])==TRUE){
      range = max(rowSums(i))-min(rowSums(i))
      if (range<=eps){result[cont] = 1}
    }
    cont=cont+1
  }
  return(result)
}
```

We are doing a tiny example for a simple vector which cannot be compositional data by definition, a matrix which apparently is a compositional vector and a matrix that is not. 



```{r}
m1 = matrix(c(0.25,0.249999,0.2,0.2))
m1 <- cbind(m1, c(0.3, 0.3, 0.3, 0.35))
m1 <- cbind(m1, c(0.45, 0.45, 0.5, 0.45)) # definig the comp matrix

m2 = matrix(c(0.25,0.249999,0.2,0.2))
m2 <- cbind(m2, c(0.3, 0.9, 0.3, 0.35))
m2 <- cbind(m2, c(0.45, 0.45, 0.5, 0.65)) # definig the non comp matrix

v1 = c(1,2,2,1) # simple vector
lex = list(v1,m1,m2)
```

Vector:
```{r}
v1
```

```{}
[1] 1 2 2 1
```


Compositional Matrix: 
```{r}
m1
```

```{}
         [,1] [,2] [,3]
[1,] 0.250000 0.30 0.45
[2,] 0.249999 0.30 0.45
[3,] 0.200000 0.30 0.50
[4,] 0.200000 0.35 0.45
```


Non Compositional Matrix: 
```{r}
m2
```

```{}
         [,1] [,2] [,3]
[1,] 0.250000 0.30 0.45
[2,] 0.249999 0.90 0.45
[3,] 0.200000 0.30 0.50
[4,] 0.200000 0.35 0.65
```



Results if no deviation for the sum of the components is allowed:
```{r}
which_coda(lex,0.1)
```

```{}
[1] 0 0 0
```

```{r}
m2
```



Results if a tiny (0.001) deviation for the sum of the components is allowed:
```{r}
which_coda(lex,111)
```

```{}
[1] 0 1 0
```


### Function 2. Closure operator.

This function returns the closed form of a given compositional vector (given as a matrix).

```{r}
C=function(x, k){ # x the matrix of the variable, k the constant (=1 if missing)
  if (missing(k)){
    newx <- (x)/rowSums(x)
    return(newx)
  }
  else{
    newx <- (k*x)/rowSums(x)
    return(newx)
  }
} 
  
```

An example with the previously defined "m2" FOR $k=1$:
```{r}
C(m2)
```

```{}
          [,1]      [,2]      [,3]
[1,] 0.2500000 0.3000000 0.4500000
[2,] 0.1562495 0.5625004 0.2812502
[3,] 0.2000000 0.3000000 0.5000000
[4,] 0.1666667 0.2916667 0.5416667
```




An example with the previously defined "m2" FOR $k=100$:
```{r}
C(m2,100)
```

```{}
         [,1]     [,2]     [,3]
[1,] 25.00000 30.00000 45.00000
[2,] 15.62495 56.25004 28.12502
[3,] 20.00000 30.00000 50.00000
[4,] 16.66667 29.16667 54.16667
```


### Function 3. Naive Ternary plot (from scratch).

Here is the code for plotting ternary diagrams. Its input is a matrix with the compositional data and its output the aforementioned plot. Of course, it is very naive -yet-.

```{r}

ternary=function(mat){ # depends only in the data matrix containing the compositional datA
  x = mat[1,][1]
  y = mat[1,][2]
  z = mat[1,][3]
  p=c(0.5*(2*y+z)/(x+y+z), sqrt(3/2)*(z/(x+y+z)))
  plot(p[1],p[2],xlim=c(0,1),ylim=c(0,1), pch=20,xlab=" ",ylab=" ", cex=0.5)
  if (nrow(mat)>1){
    for (i in 2:nrow(mat)){
    x = mat[i,][1]
    y = mat[i,][2]
    z = mat[i,][3]
    p=c(0.5*(2*y+z)/(x+y+z), sqrt(3/2)*(z/(x+y+z)))
    points(p[1],p[2],pch=20, cex=0.5)
    }
  }
  segments(0,0,1,0)
  segments(0.5,sqrt(3/4),0,0)
  segments(0.5,sqrt(3/4),1,0)
}
```


```{r,fig.height=5,fig.width=5}
ternary=function(mat, leg){ # depends only in the data matrix containing the compositional datA
  if (missing(leg)){leg = c("x1","x2","x3")}
  x = mat[1,][1]
  y = mat[1,][2]
  z = mat[1,][3]
  p=c(0.5*(2*y+z)/(x+y+z), sqrt(3/2)*(z/(x+y+z)))
  plot(p[1],p[2],xlim=c(-0.1,1.1),ylim=c(-0.1,1), pch=20,xlab=" ",ylab=" ", cex=0.8,frame.plot = FALSE, axes=FALSE)
  if (nrow(mat)>1){
    for (i in 2:nrow(mat)){
    x = mat[i,][1]
    y = mat[i,][2]
    z = mat[i,][3]
    p=c(0.5*(2*y+z)/(x+y+z), sqrt(3/2)*(z/(x+y+z)))
    points(p[1],p[2],pch=20, cex=0.8)
    }
  }
  text(-0.04, -0.04, leg[1],col="darkblue")
  text(1.04, -0.04, leg[2],col="darkgreen")
  text(0.5, 0.91, leg[3],col="darkred")
  segments(0,0,1,0,col="darkgreen")
  segments(0.5,sqrt(3/4),0,0,col="darkblue")
  segments(0.5,sqrt(3/4),1,0,col="darkred")
  
  redlist = list(c(0.2,0.8),c(0.4,0.6),c(0.6,0.4),c(0.8,0.2))
  for (i in redlist){
      x = i[1]
      y = 0
      z = i[2]
      p1=c(0.5*((2*y+z)/(x+y+z)), sqrt(3/4)*(z/(x+y+z)))
      text(p1[1]-0.03, p1[2]+0.03, i[1]*100,col="darkblue",cex=0.8,srt=-60)
      x = 0
      y = i[1]
      z = i[2]
      p2=c(0.5*((2*y+z)/(x+y+z)), sqrt(3/4)*(z/(x+y+z)))
      segments(p1[1],p1[2],p2[1],p2[2],col="darkred",lty="dotdash")
  }
  for (i in redlist){
      x = i[1]
      y = 0
      z = i[2]
      p1=c(0.5*((2*y+z)/(x+y+z)), sqrt(3/4)*(z/(x+y+z)))
      x = i[1]
      y = i[2]
      z = 0
      p2=c(0.5*((2*y+z)/(x+y+z)), sqrt(3/4)*(z/(x+y+z)))
      text(p2[1]-0.03, p2[2]-0.03, i[1]*100,col="darkgreen",cex=0.8,srt=60)
      segments(p1[1],p1[2],p2[1],p2[2],col="darkblue",lty="dotdash")
  }
  for (i in redlist){
      x = 0
      y = i[2]
      z = i[1]
      p1=c(0.5*((2*y+z)/(x+y+z)), sqrt(3/4)*(z/(x+y+z)))
      text(p1[1]+0.05, p1[2], i[1]*100,col="darkred",cex=0.8,srt=0)
      x = i[1]
      y = i[2]
      z = 0
      p2=c(0.5*((2*y+z)/(x+y+z)), sqrt(3/4)*(z/(x+y+z)))
      segments(p1[1],p1[2],p2[1],p2[2],col="darkgreen",lty="dotdash")
  }
}
```

The ternary function has been updatef following the Thibault code in 'Analyzing the impacts of socio-economic factors on French departmental elections with CoDa methods (Supplementary material)':

```{r, fig.width=5,fig.height=5}
ternary(m2,c("Renault","Jaguar","Seat"))
```

**2 ISSUES: EXTERNAL ARROWS (less important), KEEP THE FIG SIZE! (here it is adjusted through the Rmarkdown opts)**


I will test the 'compositions' ternary diagram:

![Figure 1: Ternary plot 2](https://raw.githubusercontent.com/i-rb/coda/main/img/ternary2.png)

```{r}
ac=acomp(m2)
wd=getwd()
png(filename=paste0(wd, "/img/ternary2.png"))
plot(ac)
dev.off()
```

_Surprisingly, it doesn't make that much difference..._

## 1.2.1 The Aitchison Geometry (Operations)
### Function 4. The perturbation.

The perturbation in the Aitchison geometry is defined as $\mathbf{x}\oplus\mathbf{p} = \mathcal{C}[x_1p_1,...,x_Dp_D]$:

```{r}
pt = function (x,y){ # I prefer to keep an easy name for this kind of functions, x and y are matrix 1xD
  return(C(x*y))
}
```


We define $x_1$ and $x_2$ and use the perturbation operation.

```{r}
x1 = t(matrix(c(0.3,2,1,3)))
x2 = t(matrix(c(3,  1,2,4)))

pt(x1,x2)
```

```{}
           [,1]      [,2]      [,3]      [,4]
[1,] 0.05325444 0.1183432 0.1183432 0.7100592
```


We can see that $x_1 \oplus x_2$ lies in the simplex.
```{r}
sum(pt(x1,x2)) # arreglar PT
```

```{}
[1] 1
```




Here I replicate the example given in the "Transformations" document:

```{r}
pt(t(matrix(c(1,2,3))),t(matrix(c(1,2,1))))
```

```{}
      [,1] [,2]  [,3]
[1,] 0.125  0.5 0.375
```


### Function 5. The powering.

The powering function is defined by  $\lambda \odot \mathbf{x} = \mathcal{C}[x_1^\lambda,...,x_D^\lambda]$. 

```{r}
pw=function(l,x){ # x the vector (matrix 1xD) and l stands for the scalar
  return(C(x**l))
}
```

As an example, $2\odot x_1$, which should coincide with $x_1\oplus x_1$:

```{r}
pw(2,x1) # powering
```

```{}
            [,1]      [,2]       [,3]      [,4]
[1,] 0.006387509 0.2838893 0.07097232 0.6387509
```


```{r}
pt(x1,x1) # perturbation
```

```{}
            [,1]      [,2]       [,3]      [,4]
[1,] 0.006387509 0.2838893 0.07097232 0.6387509
```



... and  the "transformations" document example:

```{r}
pw(2,t(matrix(c(1,2,1)))) # perturbationx
```


```{}
          [,1]      [,2]      [,3]
[1,] 0.1666667 0.6666667 0.1666667
```


### Function 6. The Aitchison inner product.

The inner product of this geometry is defined as: $\langle \mathbf{x}, \mathbf{y} \rangle_A = \dfrac{1}{D} \sum_{i=1}^{D-1}\sum_{j=i+1}^{D} \ln\dfrac{x_i}{x_j}\ln\dfrac{y_i}{y_j}$. 

```{r}
inner = function(x,y){ # x, y are Dx1 matrices
  result = 0
  D = ncol(x)
  for (i in (1:(D-1))){
    result <- result + sum(log(x[i]/x[(i+1):D])*(log(y[i]/y[(i+1):D])))
  }
  return((1/D)*result)
}
```

Thus, $\langle x_1,x_2 \rangle=-0.26672$

```{r}
inner(x1,x2)
```

```{}
[1] -0.2667037
```


### Function 7. The Aitchison distance.

The distance is defined as $d_A(\mathbf{x},\mathbf{y}) = \left(\dfrac{1}{D} \sum_{i=1}^{D-1}\sum_{j=i+1}^{D} \left(\ln\dfrac{x_i}{x_j}-\ln\dfrac{y_i}{y_j}\right)^2\right)^{1/2}$

```{r}
dist = function(x,y){ # x,y 1xD matrices
  result = 0
  D = ncol(x)
  for (i in (1:(D-1))){
    result <- result + sum((log(x[i]/x[(i+1):D])-(log(y[i]/y[(i+1):D])))**2)
  }
  return(((1/D)*result)**(1/2))
}
```

As any distance, $d_A(x_1,x_1)$ should be equal to 0 and positive otherwise:

```{r}
dist(x1,x1)
```
```{}
[1] 0
```



```{r}
dist(x1,x2)
```

```{}
[1] 2.1606
```


I cannot get the equality between norm(x-y) and distance(x,y), despite it is stated in the page (p.41) of the transformations document (maybe **only a feature of the 'compositions' package**? Yes! That is!). Empirically, I even get an error because $z:=x-y$ has positive and negative components and, therefore, $ln(z_i/z_j)$ should be calculated and $\exists\; i,j$ such that $z_i\cdot z_j<0$ 

```{r}
dist(x1,x2)
```

```{}
[1] 2.1606
```


```{r}
sqrt(inner(x1-x2,x1-x2))
```

```{}
NaNs producedNaNs producedNaNs producedNaNs produced[1] NaN
```


```{r}
x1c = acomp(x1)   
x2c = acomp(x2)
norm(x1c-x2c) # composition package norm
```

```{}
[1] 2.1606
```



### Function 8. Linear operator.

The linear operator is defined as $A\boxdot\mathbf{x} = \mathcal{C}\left(\prod_{j=1}^D x_j^{a_{1j}},...,\prod_{j=1}^D x_j^{a_{Dj}}\right)^T$ for $A$ a DxD matrix and $x$ a 1xD vector.

There are certain properties that the matrix $A$ should meet in order to represent a endomorphism in $\mathcal{S}^D$. This first function check them (there are 4).

```{r}
check_endo = function(A){ # A is the matrix
  if (!(is.numeric(A))){return(FALSE)} # it is a real matrix
  if (!(dim(A)[1]==dim(A)[2])){return(FALSE)} # it is a square matrix
  if (!(sum(round(rowSums(A),3)==(numeric(dim(A)[1])))==(dim(A)[1]))){return(FALSE)} # rows sum 0
  if (!(sum(colSums(A)==(numeric(dim(A)[1])))==(dim(A)[1]))){return(FALSE)} # cols sum 0 
  # if (!((qr(ilr(A))$rank)==(qr(A)$rank))){return(FALSE)} # rank(A)==rank(A*) I CANNOT GET THIS nor
  # ilr(A) == Id_D-1 (CANNOT GET THE ILR)
  return(TRUE)
}
```


```{r}
lin = function (A, x){ # A matrix, x vector 1xD
  if (!(check_endo(A))){
    print("Non suitable endomorphism.")
    return(FALSE)}
  vec=numeric(length(x))
  c=0
  for (i in 1:nrow(A)){
    c=c+1
    vec[c]=prod(x[i]**A[i,])
  }
  return(C(t(vec)))
}
```

Here the function is tested:

```{r}
  v3=matrix(c(0.4,0.2,0.4)) # defining a vector

  m9 = matrix(c(1/3,-2/3,1/3))
  m9 <- cbind(m9, c(1/3,1/3,-2/3)) # defining a square matrix
  m9 <- cbind(m9, c(-2/3,1/3,1/3)) 

  lin(m9,v3)
```

```{r}
lin(m1,v3)
```

```{}
          [,1]      [,2]      [,3]
[1,] 0.4221787 0.1926071 0.3852142
```


## 1.2.2 The Aitchison Geometry (Transformations)
### Function 9. The alr transfomration.
The alr transformation is given by: $\mathrm{alr}(\mathbf{x})=(\ln(x_1/x_D),...,\ln(x_{D-1}/x_D))$

```{r}
alr = function(x,i){   # x is a 1xD matrix, i an integer in (1,D): the "reference" element
  if (missing(i)){     # if i is not given, the last element is used 
    return(log(x/x[length(x)])[-length(x)])
  }
  else{
    return(log(x/x[i])[-i])
  }
}
```

For example, the transformation for $x_1$:

```{r}
alr(x1)
```

```{}
[1] -2.3025851 -0.4054651 -1.0986123
```

And it coincides with the result of the 'compositions' package:

```{r}
compositions::alr(x1)[1,]
```

```{}
[1] -2.3025851 -0.4054651 -1.0986123
```


### Function 10. The ilr transformation.

The ilr transformation is given by:

$\mathrm{ilr}(\mathbf{x})=\mathbf{V}^T_D\ln(\mathbf{x}):=x^*$

For the 3-component case (the function should be extended if needed), the matrix $\mathbf{V}_3$ is given by: $\mathbf{V}_3 = \left( \begin{matrix} \sqrt\frac{2}{3} & 0 \\ -\frac{1}{\sqrt6} & \frac{1}{\sqrt2} \\ -\frac{1}{\sqrt6} & -\frac{1}{\sqrt2}\end{matrix}\right)$

```{r}
ilr = function(x){ # x matrix 1xD as always (deprecated)
  V3 = matrix(c(sqrt(2/3),-sqrt(1/6),-sqrt(1/6)))
  V3 <- cbind(V3, c(0,sqrt(1/2),-sqrt(1/2))) # defining V3
  return(t(V3)%*%log(x))
}
```

Here, the alr transformation is used for a new vector $x_3$:

```{r}
x3=c(0.4,0.2,0.4)
ilr(x3)

```
```{}
           [,1]
[1,]  0.2829762
[2,] -0.4901291
```


### Function 11. The ilr inverse.

For this, the exponential of a matrix is needed. I will compute a naive Taylor-series based one (in order to avoid using external packages for the moment):

```{r}
expm = function(M, iter){ # matrix and number of iterations (the more the better)
  if (missing(iter)){     # if i is not given, the last element is used 
    iter=100
  }
  res=diag(length(M[1,]))
  power=M
  for (i in 1:iter){
    res = res + power/factorial(i)
    power = power%*%M
  }
  return(res)
}
```

It works quite well!
```{r} 
test1 <- t(matrix(c(4, 2, 0,1, 4, 1,1, 1, 4), 3, 3))
expm(test1,100)
```

```{}
         [,1]     [,2]      [,3]
[1,] 147.8666 183.7651  71.79703
[2,] 127.7811 183.7651  91.88257
[3,] 127.7811 163.6796 111.96811
```



Of course, this was not needed. I did not realise the dimensions.

Now, the inverse ilr (only for the D=3 case): $x=\mathrm{ilr}^{-1}(\mathbf{x}^*) = \mathcal{C}(\exp(\mathbf{V}_D x^*)$ 

```{r}
inverseilr=function(x){ # x is x*, vec of coordinates
  V3 = matrix(c(sqrt(2/3),-sqrt(1/6),-sqrt(1/6)))
  V3 <- cbind(V3, c(0,sqrt(1/2),-sqrt(1/2))) # defining V3
  return(C(t((exp(V3%*%x)))))
}
```


Testing the new function for $x_3$, we should have $\mathrm{ilr}^{-1}(\mathrm{ilr(\mathbf{x})})=\mathbf{x}$. Perfect! _(there was a bug in previous versions, I forgot a "log" somewhere)_

```{r}
x3
```

```{}
[1] 0.4 0.2 0.4
```


```{r}
inverseilr(ilr(x3))
```

```{}
[1] 0.4 0.2 0.4
```


Now that I have read in the transformations document the explicit formula of $\mathbf{V}$ I can make a function to compute it here:

```{r}
V_D = function(D){ # D is the dimension (parts)
  V_D = matrix(0L, nrow =D , ncol = D-1)
  for (i in 1:(D-1)){  # pseudodiagonal elements
    V_D[i,i] = (D-i)/(sqrt((D-i+1)*(D-i)))
  }
  for (i in 2:D){ # below pseudodiagonal elements
    for (j in 1:i-1){
      V_D[i,j]=-1/(sqrt((D-j+1)*(D-j)))
    }
  } 
  return(V_D)
}
```

The matrix $V_3$ had been computed previously. It is checked that they coincide:

```{r}
V3 = matrix(c(sqrt(2/3),-sqrt(1/6),-sqrt(1/6)))
V3 <- cbind(V3, c(0,sqrt(1/2),-sqrt(1/2))) # defining V3 as before
V3

V_D(3) # our brand new function
```

```{}
           [,1]       [,2]
[1,]  0.8164966  0.0000000
[2,] -0.4082483  0.7071068
[3,] -0.4082483 -0.7071068
           [,1]       [,2]
[1,]  0.8164966  0.0000000
[2,] -0.4082483  0.7071068
[3,] -0.4082483 -0.7071068
```


Now, it is possible to update the ilr function: 

```{r}
ilr = function(x){ # x matrix N x D
  x = C(x)
  V = V_D(dim(x)[2]) 
  return(t(t(V)%*%t(log(x))))
}
```


```{r}
testvec = t(matrix(c(1,2,4)))
ilr(testvec)
```

```{}
           [,1]       [,2]
[1,] -0.8489285 -0.4901291
```

However... (I can assume the first and the second coordinates are interchangables, and also that (x,y), (-x, -y) are equivalent representations, but...). I have found that the 'compositions' package what does here is $\mathrm{ilr}(x) = V^T \mathrm{clr}(x)$.

```{r}
ac = acomp(c(1,2,4))

compositions::ilr(ac)
```

```{}
          [,1]      [,2]
[1,] 0.4901291 0.8489285
attr(,"class")
[1] "rmult"
```


Here I am trying to recover the result of the compositions package by permuting the $V_D$ matrix (for the easy D=3 case):

```{r}
ilr2 = function(x){ # x matrix N x 3 (only works with D=3)
  x = C(x)
  V = V_D(dim(x)[2])
  V2 = cbind(V_D(dim(x)[2])[,2],V_D(dim(x)[2])[,1])
  return(t(t(V2)%*%t(log(x))))
}
```

With the simple permutation, it is achieved the same order in the coordinates.

```{r}
ilr2(testvec)
```


```{r}
ilr3 = function(x){ # x matrix N x 3 (only works with D=3)
  x = C(x)
  V = V_D(dim(x)[2])
  V2 = -1*cbind(V_D(dim(x)[2])[,2],V_D(dim(x)[2])[,1])
  return(t(t(V2)%*%t(log(x))))
}
```

And multiplying by -1 we get it:

```{r}
ilr3(testvec)
```


What is happening in the 'compositions' package? They use different methods, but not ours. This is their usual one. It seems that they construct every $V_D$ from the $V_2$ and theirs is the same as our $V_2$ but permuting their elements. (I.e.: theirs: $(-0.7071,0.7071)$ and ours $(0.7071,-0.7071)$.)

```{} 
function (W = c(1, -1)) # gsi.buildilrBase
{
    if (length(W) < 2) {
        return(ilrBase(D = 1))
    }
    if (length(dim(W)) == 0) {
        return(ilrBase(D = 2))
    }
    if (length(dim(W)) > 0) {
        W = as.matrix(W)
        nc = ncol(W)
        D = nrow(W)
        isPos = (W > 0)
        isNeg = (W < 0)
        nPos = matrix(1, D, D) %*% isPos
        nNeg = matrix(1, D, D) %*% isNeg
        W = (isPos * nNeg - isNeg * nPos)
        nn = sapply(1:nc, function(i) {
            1/norm.rmult(W[, i])
        })
        nn = matrix(nn, ncol = ncol(W), nrow = nrow(W), byrow = TRUE)
        W = W * nn
        return(W)
    }
}
```


#### ILR Inverse

Now we should also update the ilr inverse, that we will need for the regressions section:

```{r}
inverseilr=function(x){ # x is x*, matrix of coordinates
  Vi = V_D(dim(x)[1]+1)
  return(C(t(exp(Vi%*%x))))
}
```

It works!! (There is an issue: ilr(m1) instead of the matrix of coordinates returns the transpose of the matrix of coordinates)
```{r}
inverseilr(t(ilr(m1)))
```


```{}
          [,1]      [,2]      [,3]
[1,] 0.2500000 0.3000000 0.4500000
[2,] 0.2499992 0.3000003 0.4500005
[3,] 0.2000000 0.3000000 0.5000000
[4,] 0.2000000 0.3500000 0.4500000

```



```{r}
m1
```

```{}
         [,1] [,2] [,3]
[1,] 0.250000 0.30 0.45
[2,] 0.249999 0.30 0.45
[3,] 0.200000 0.30 0.50
[4,] 0.200000 0.35 0.45
```




<a name="link1">

## 1.3. Regressions (to be complated below)

</a>

The good news is that the ilr transformation defined previously works with (the transpose of) matrices; the bad is that the regression cannot be done yet (remark that both the independent and dependent variables could be matrices). 

```{r}
ilr(t(m2))
```

```{}
           [,1]       [,2]       [,3]
[1,]  0.1288331  0.1821927  0.0000000
[2,] -0.3616415  0.8340814 -0.1090010
[3,] -0.1365680 -0.1931363 -0.1855196
```


## 1.4. Descriptive Analysis of Compositional Data

### Function 12. The compositional mean.

The compositional mean of a variable $\mathbf{x}$ is given by:

$\bar{\mathbf{x}} = \dfrac{1}{N} \odot \bigoplus^N_{n=1}\mathbf{x}_n = \mathcal{C}\left(\exp\left(\dfrac{1}{N}\sum_{n=1}^N \ln(\mathbf{x}_n) \right)\right)$


```{r}
c_mean=function(x){ #x is a matrix NxD
  mult = t(matrix(x[1,]))
  for (i in (2:(dim(x)[1]))){
    mult=pt(mult,t(matrix(x[i,])))
  }
  res = pw(1/dim(x)[1], mult)
  return(res)
}
```

For example:

```{r}
c_mean(m1)
```

```{}
          [,1]      [,2]     [,3]
[1,] 0.2241885 0.3125984 0.463213
```


And it coincides with the 'compositions' package mean:

```{r}
mean(acomp(m1))
```

```{}
[1] 0.2241885 0.3125984 0.4632130
attr(,"class")
[1] acomp
```


We can visualise it. It should be the center of the other points. _In this ternary diagramm, the mean is represented by point that is surrounded by the other three. It does not look good._

![Figure 3: Ternary plot 3](https://raw.githubusercontent.com/i-rb/coda/main/img/ternary3.png)

```{r,fig.width=2, fig.height=2}
wd=getwd()
png(filename=paste0(wd, "/img/ternary3.png"))
ternary(rbind(m1,c_mean(m1)))
dev.off()
```


### Function 13. The total variance.

The total variance for a compositional variable $\mathbf{x}$ is defined as:

$\mathrm{tvar}(\mathbf{x})=\dfrac{1}{N-1}\sum_{n=1}^N d_A^2(\mathbf{x}_n, \bar{\mathbf{x}})$

```{r}
tvar = function(x){ # x is a NxD matrix
  m = c_mean(x)
  suma=0
  N = dim(x)[1]
  for (i in 1:N){
    suma = suma + (dist(t(matrix(x[i,])),m))**2
  }
  return(suma/(N-1))
}
```

It coincides with the 'compositional' package result for $m_1$:

```{r}
tvar(m1)
```

```{}
[1] 0.02421196
```


```{r}
mvar(acomp(m1))
```

```{}
[1] 0.02421196
```


### Function 14. The Standard Deviation.

The SD for compositional data can be defined through the total variance as:

$\mathrm{tsd}(\mathbf{x})=\sqrt{\dfrac{1}{D-1}\mathrm{tvar}(\mathbf{x})}$

```{r}
csd = function(x){ # x NxD matrix
  D = dim(x)[2]
  return(sqrt(tvar(x)/(D-1)))
}
```

E.g.:

```{r}
csd(m1)
```

```{}
[1] 0.1100272
```



### Function 15. Variation Matrix.

The Variation Matrix is defined empirically as a DxD matrix with components:

$\hat{\tau}_{ij} = \dfrac{1}{N-1}\sum_{n=1}^N \ln^2 \dfrac{x_{ni}}{x_{nj}} - \ln^2 \dfrac{\bar{x_i}}{\bar{x_j}}$

```{r}
varm = function(x){ # x is a NxD matrix
  N=dim(x)[1]
  D=dim(x)[2]
  m = c_mean(x)
  res = matrix(0,D,D)
  for (i in 1:D){
    for (j in 1:D){
      res[i,j]=(1/(N-1))*(sum((log(x[,i]/x[,j]))**2))-(log(m[1,i]/m[1,j]))**2
    }
  }
  return(res)
}
```

If we compute it for $m_1$, it should coincide with the 'compositions' package variation:

```{r}
varm(m1)
```

```{}
           [,1]       [,2]       [,3]
[1,] 0.00000000 0.07084081 0.20275598
[2,] 0.07084081 0.00000000 0.06297584
[3,] 0.20275598 0.06297584 0.00000000
```



```{r}
compositions::variation(acomp(m1))
```

```{}
           [,1]      [,2]      [,3]
[1,] 0.00000000 0.03400380 0.02720936
[2,] 0.03400380 0.00000000 0.01142272
[3,] 0.02720936 0.01142272 0.00000000
```


But it does not! Why? Because 'compositions' uses the direct version: $\tau_{ij}=V(\ln(\frac{x_i}{x_j}))$.

```{r}
varm2 = function(x){ # x is a NxD matrix
  N=dim(x)[1]
  D=dim(x)[2]
  m = c_mean(x)
  res = matrix(0,D,D)
  for (i in 1:D){
    for (j in 1:D){
      res[i,j]=var(log(x[,i]/x[,j]))
    }
  }
  return(res)
}
```

```{r}
varm2(m1) # this coincides
```

```{}
           [,1]       [,2]       [,3]
[1,] 0.00000000 0.03400380 0.02720936
[2,] 0.03400380 0.00000000 0.01142272
[3,] 0.02720936 0.01142272 0.00000000
```


### Function 16. Centering data.

The transformation from $\mathbf{x}$ to the transformed $\mathbf{z}$ is done through:

$\mathbf{z} = \dfrac{1}{\sqrt{\mathrm{mvar}(X)}}\odot(\mathbf{x-\bar{\mathbf{x}}})$

```{r}
center = function (x){ # x is an NxD matrix
  denominator = sqrt(tvar(x))
  resta = -matrix(c_mean(x), nrow=dim(x)[1], ncol=dim(x)[2], byrow=TRUE)
  res = pw(denominator,pt(x,resta))
  return(res)
}
```

It works! However, as the sample size is only N=4 with similar points, the result looks strange. 

```{r}
center(m1)
```

```{}
          [,1]      [,2]      [,3]
[1,] 0.3021005 0.3272931 0.3706064
[2,] 0.3021003 0.3272932 0.3706065
[3,] 0.2930169 0.3286681 0.3783150
[4,] 0.2924825 0.3360329 0.3714846
```

![Figure 4: Ternary plot 4](https://raw.githubusercontent.com/i-rb/coda/main/img/ternary4.png)

```{r,fig.height=2,fig.width=2}
wd=getwd()
png(filename=paste0(wd, "/img/ternary4.png"))
ternary(center(m1))
dev.off()
```

Here is plotted some confidence regions of different level for the center. In the 'descriptive' document there is information about the right choice for the radius $r$ and the assumptios to be hold: normal data, N large... 

However, there is no info about the ellipses construction.May it be the inverse of the mlr of a Sphere? _This could be checked later if needed_

![Figure 5: Ternary plot 5](https://raw.githubusercontent.com/i-rb/coda/main/img/ternary5.png)

```{r}
mm1 = mean(acomp(m1))
#plot(mm1,add=TRUE)
wd=getwd()
png(filename=paste0(wd, "/img/ternary5.png"))
plot(mm1)
ellipses(mean(acomp(m1)),variation(acomp(m1)),1)
ellipses(mean(acomp(m1)),variation(acomp(m1)),2)
ellipses(mean(acomp(m1)),variation(acomp(m1)),5)
ellipses(mean(acomp(m1)),variation(acomp(m1)),10)
ellipses(mean(acomp(m1)),variation(acomp(m1)),15)
dev.off()

```

### Function 17. Balance between two groups.

Given a comp. variable $\mathbf{x}$, for each observation, the balance between two groups is given by:

$K \sum_{a\in A} N_b \ln x_a -K \sum_{b\in B} N_B \ln x_b\;\;\;$, where $\;\;\;K=\left(N_aN_b(N_a+N_b)\right)^{-1/2}$.

```{r}
balance = function(x,A,B){ # x matrix NxD (output Nx1), A, B vector of positions of each group
  if (length(A)==1){gA = matrix(x[,A])}
  if (length(A)>1){gA = (x[,A])}
  if (length(B)==1){gB = matrix(x[,B])}
  if (length(B)>1){gB = (x[,B])}
    
  Na = dim(gA)[2]
  Nb = dim(gB)[2]
  K = (Na*Nb*(Na+Nb))**(-0.5)
  res = numeric(dim(x)[1])
  for (t in (1:dim(x)[1])){
    res[t] = K*sum(Nb*log(gA[t,])) - K*sum(Na*log(gB[t,])) 
  }
  return(matrix(res))
}
```

We can observe that it works: although the second part of $m_1$ is not more important than the "mean" of the two others, for the forth observation it is.

```{r}
m1
```

```{}
         [,1] [,2] [,3]
[1,] 0.250000 0.30 0.45
[2,] 0.249999 0.30 0.45
[3,] 0.200000 0.30 0.50
[4,] 0.200000 0.35 0.45
```


```{r}
#balance
balance(m1,c(2),c(1,3))
```

```{}
            [,1]
[1,] -0.09109797
[2,] -0.09109634
[3,] -0.04301325
[4,]  0.12586350
```

# 2. Multivariate General Linear Model

## 2.2 Estimation

### Function 18. Estimation of the coefficients.

Given the model: $\begin{pmatrix} y_1\\ y_2\\ ...\\ y_{p}\end{pmatrix} = D\begin{pmatrix}\beta_1\\\beta_2\\...\\ \beta_{p}\end{pmatrix} + \begin{pmatrix}u_1\\u_2\\...\\ u_{p} \end{pmatrix}$

The BLUE estimator of $B$ is obtained computing: 

$\hat{B} = (X^TX)^{-1}X^TY$

```{r}
mlm = function(Y,X){ # only the dependent and independent matrices are needed
  ## use solve(t(X),Y) or QR algorithm qr.solve(), cholesky (I HAVE USED QR)
  B = qr.solve(t(X)%*%X,t(X)%*%Y)
  return(round(B,8))
}
```

We have already defined $m_1$ and $m_2$, extending the latter, we can use them in the example:

```{r}
m4 = cbind(m2,c(1,2,3,4))
mlm(m1,m4)
```


```{}
     [,1]        [,2]       [,3]
[1,]    1  0.69642776  3.7499956
[2,]    0 -0.01785596 -0.2499936
[3,]    0  0.26785683 -1.2500017
[4,]    0  0.01071427  0.1499999
```

### Function 18Bis. Adding covariances.

Now that we have this, we can now complete the [regressions section](#link1). However, it will be completed here (in order to keep the order of computation):

## 1.3. Regressions (BIS)
### Function 19. Complete Regression.

Assume we have $k$ explanatory variables (some compositional and some not) and $N$ observations. The dependent variable $Y$ can be compositional or not. In this first version there are four inputs: $Y$, the independent variable which can be compositional (comp=1) or not (comp=0), X is the list of non compositional variables and Z the list of compositional variables. Each variable inside the list is a matrix nxD (Dâ‰¥1 the number of parts).

Recall that the regression in the ILR coordinate space is defined by:

$\mathrm{ilr}(\mathbf{Y}_i) = \mathbf{b}_0^*+ \sum_{q=1}^Q\mathrm{ilr}(\mathbf{X}_{qi})\mathbf{B}^*_q + \sum_{k=1}^K Z_{ki} \mathbf{b}^*_k + \mathrm{ilr}(\epsilon_i)$

```{r}
  c_lm = function(Y,X,Z,comp){ 
    if (missing(X)){
      X=list(matrix())
    }
    if (missing(Z)){
      Z=list(matrix())
    }
    if (comp==1){
      ty = ilr(Y)
    }
    else{
      ty = Y
    }
    tx = matrix(NaN,dim(Y)[1])
    if (!is.na(X[[1]][1,1])){
      for (var in X){
        tx = cbind(tx,compositions::ilr(var)) ## here compositions::ilr to recover their result
      }
    }
    if (!is.na(Z[[1]][1,1])){
      for (var in Z){
        tx = cbind(tx,var)
      }
    }
    tx = tx[,-1]
    print("Transformed endogenous:")
    print(ty)
    print("--------------------------")
    print("Transformed exogenous")
    print(tx)
    print("--------------------------")
    
    res = mlm(ty,tx)
    print("B matrix in the ILR space:")
    print(res)
    print("--------------------------")
    print("B in the simplex:")
    print(inverseilr(t(res)))
  }
```

However... I do not get the same results (not even near). The divergence arises in the ilr function If I change the ilr functions to the 'compositions' package ones ONLY IN THE EXOGENOUS VARIABLE CASE, I can recover they results.



```{r}
m1 = matrix(c(0.25,0.249999,0.2,0.2))
m1 <- cbind(m1, c(0.2, 0.99, 1.2, 0.8))
m1 <- cbind(m1, c(0.4, 0.15, 1, 0.9)) # definig the comp matrix

m2 = matrix(c(0.25,0.23,0.2,0.8))
m2 <- cbind(m2, c(0.3, 0.9, 0.9, 0.35))
m2 <- cbind(m2, c(1.2, 0.45, 0.5, 0.65)) # definig the non comp matrix

a<-list(m1)

c_lm(Y=m1,X=list(m2),comp=1) #there is no Z
```

```{r}
compositions::ilrInv(lm(compositions::ilr(m1)~0+compositions::ilr(m2))$coefficients)
```

Moreover:

```{r}
inverseilr(t(ilr(m2)))
```

```{r}
compositions::ilrInv(compositions::ilr(m2))
```
```{r}
C(m2)
```

```{r}
m1c = compositions::acomp(m1)
m2c = compositions::acomp(m2)
```

```{r}
lm(m1c ~ 0 + m2c)
```

... This coincides with the "standard" multivariate regression (without transforming the variables to the coordinate space). I think theirs is not complete.

```{r}
mlm(C(m1),C(m2)) # but this is the direct regression. Not taking into account the transformation and the inverse transformation.
```



I should add the intercept case.


### Function 20. Variance-covariance matrices.

In order to obtain the estimator of the variance matrix of the parameters, first it is needed to estimate the common covariance structure $\Sigma$. Recall that $\mathrm{cov}(Y)=I_n\otimes \Sigma$. Then: 

$\hat{\Sigma}=\dfrac{Y'(I-X(X'X)^{-1}X')Y}{n}=\dfrac{(Y-X\hat{B})'(Y-X\hat{B})}{n}$

```{r}
est_var = function(X,Y){ # page 146 kevin
  B=mlm(X,Y)
  n=dim(Y)[1]
  sig = (1/n)*t(Y-X%*%B)%*%(Y-X%*%B)
  return(sig)
}
```

```{r}
est_var(m1,m2)
```

Now we can use the $\mathrm{cov}(\hat{\beta})$ estimator from the Kevin book:

```{r}
# p 145 Kevin (5.13)

b_cov = function(X,Y){
  sig = est_var(X,Y)
  cov = kronecker(sig,solve(t(X)%*%X))
  return(round(cov,5))
}

```

```{r}
b_cov(m1,m2)
```
 
Now it is possible to update with this info the general 'c_lm' function:

```{r} 
  c_lm = function(Y,X,Z,comp,infocov){ #infocov = TRUE if info about covariances desired (sometimes too large)
    if (missing(X)){
      X=list(matrix())
    }
    if (missing(Z)){
      Z=list(matrix())
    }
    if (comp==1){
      ty = ilr(Y)
    }
    else{
      ty = Y
    }
    tx = matrix(NaN,dim(Y)[1])
    if (!is.na(X[[1]][1,1])){
      for (var in X){
        tx = cbind(tx,compositions::ilr(var)) ## here compositions::ilr to recover their result
      }
    }
    if (!is.na(Z[[1]][1,1])){
      for (var in Z){
        tx = cbind(tx,var)
      }
    }
    tx = tx[,-1]

    print("Transformed endogenous:")
    print(ty)
    print("--------------------------")
    print("Transformed exogenous")
    print(tx)
    print("--------------------------")
    
    res = mlm(ty,tx)
    print("B matrix in the ILR space:")
    print(res)
    print("--------------------------")
    print("B in the simplex:")
    print(inverseilr(t(res)))
    if(missing(infocov)){infocov=FALSE}
    if (infocov==TRUE){
      info = b_cov(tx,ty)
      print("--------------------------")
      print("Covariance Matrix Est.:")
      print(info)
    }
  }
```

```{r}
c_lm(X=list(m2),Y=m1,comp=1,infocov=TRUE)
```

```{r}
m2
```



# 3. Creating a Class

I have never done this before but I will try to replicate the approach used by the 'compositions' package: create a class and "change" the definition of the operators within that class. E.g: The operator "+" between two arguments in the standard class (does it have a name?) will sum up as always: 2+2=4. However, if those two arguments $x$, $y$ come from the 'compositional' class (i.e. they are comp. variables), addition would mean $\oplus$. Id est, $x\oplus y := x+y$ in the code. The class represents the Aitchison geometry. 

First, we define the class and the function 'as' to convert objects to this class.

```{r} 
as.comp <- function(x){
  if(!inherits(x, "comp")) class(x) <- c("comp", class(x)) # this is to keep the original class
  C(x)
}
```

```{r}
x1c = as.comp(x1)
x2c = as.comp(x2)
x1c
```

```{}
           [,1]      [,2]      [,3]      [,4]
[1,] 0.04761905 0.3174603 0.1587302 0.4761905
attr(,"class")
[1] "comp"   "matrix"
 
```


Now, we should define the sum for this class (it is called 'overload' in R), which is the perturbation.

```{r}
`+.comp` = function(e1, e2){
  pt(e1,e2)
}
```

We can check that $x_1' + x_2' = C(x_1) \oplus C(x_2)$ where $x_i'$s belong to the new class.

```{r}
x1c+x2c
```

```{}
           [,1]      [,2]      [,3]      [,4]
[1,] 0.05325444 0.1183432 0.1183432 0.7100592
attr(,"class")
[1] "comp"   "matrix"
```


```{r}
pt(x1, x2)
```

```{}
           [,1]      [,2]      [,3]      [,4]
[1,] 0.05325444 0.1183432 0.1183432 0.7100592
```

In the compositional package class (acomp, there three/four more)... 

```{r}
acomp(x1)+acomp(x2)
```

```{}
           [,1]       [,2]      [,3]      [,4]     
[1,] 0.05325444 0.1183432 0.1183432 0.7100592
attr(,"class")
[1] acomp
```

Two differences stand out. The first is the function for defining the class. I prefer 'their' form (acomp(x1) or comp(x1) than as.comp(x1)). Second, the object created in our class is decided to have two classes, as it retains the original one. Is this better or worse?

I will correct the former. I do not know if this is the most suitable approach.

```{r}
cc = function(var){
  return(as.comp(var))
}
```

```{r}
cc(x1)
```

```{}
           [,1]      [,2]      [,3]      [,4]
[1,] 0.04761905 0.3174603 0.1587302 0.4761905
attr(,"class")
[1] "comp"   "matrix"
```

Of course, it is possible to extend all the functions already created to this class. Here, as an example, I overload the "*" operator to include the powering function.

```{r}
`*.comp` = function(e1, e2){
  pw(e1,e2)
}
```

```{r}
pw(2,cc(x1))
```

```{}
            [,1]      [,2]       [,3]      [,4]
[1,] 0.006387509 0.2838893 0.07097232 0.6387509
attr(,"class")
[1] "comp"   "matrix"
```


# 4. Example: Using the functions.

In this last section of this report, I will use most of these functions with some variables from the dataset "BDDSegX_ext.RData", which is the 'extension' of the 'BDDSegX.RData'. (Ofcourse, this dataset is not uploaded to GitHub, as it is the notebook).

I will use the simulated variables instead of the real ones, as this is what is finally going to be done to illustrate the package. (This may be uploaded)

```{r}
load("/Users/ivanrendobarreiro/Documents/m1_econometrics/internship/week1/data_db/BDDSegX_ext.RData")

simData = BDDSegX[c("SimS_A", "SimS_B", "SimS_C", "SimS_D", "SimS_E")] #keeep simulated vars
```


Each column of the dataframe kept is one of the five parts of a component variable which represents the share of the market that holds each firm.

### Function 1. Which of these variables are compositional?

If we take each of this variables separately, they cannot be compositional. However, they should be if we consider them jointly as a variable. 

```{r}
list_of_variables = list(simData$SimS_A, simData$SimS_B, simData$SimS_C, simData$SimS_D, simData$SimS_E)
which_coda(list_of_variables)
```

```{}
[1] 0 0 0 0 0
```


If considered jointly:

```{r}
list_of_variables = list(simData)
which_coda(list_of_variables, eps=0.00001)
```

```{}
[1] 1
```


### Function 2. Closed form operator.

Here we already have a compositional variable in its closed form. However, the operation could be applied anyway (remember $\mathcal{C}(\mathcal{C}(x))=\mathcal{C}(x)$). Therefore, I will choose k=100, the total sum:

```{r}
as.matrix(head(C(simData,100))) # as matrix bc of visualization in RStudio, nothing else
```

```{}
    SimS_A   SimS_B   SimS_C   SimS_D   SimS_E
1 6.927409 37.20303 31.69218 14.99224 9.185135
2 7.309030 32.95972 35.92120 15.57412 8.235921
3 6.434496 30.51185 35.79021 18.55172 8.711717
4 5.828921 38.77071 34.24847 15.93945 5.212451
5 6.788980 36.57509 35.83543 14.24930 6.551202
6 5.915786 36.47729 37.25225 12.94232 7.412360
```


### Function 3. Naive Ternary plot (from scratch).

This makes no sense. Instead of this, a matrix plot of ternary diagrams should be used, but I have not computed it (yet).


### Function 4. The perturbation.

First, we tranform the simData object to a 'comp' class object (say $y$). Then, we compute $y\oplus y$.

```{r}
comp_simData = cc(simData) 
head(as.matrix(comp_simData + comp_simData))
```

```{}
        SimS_A    SimS_B    SimS_C    SimS_D    SimS_E
[1,] 0.1385482 0.7440606 0.6338436 0.2998448 0.1837027
[2,] 0.1461806 0.6591945 0.7184241 0.3114824 0.1647184
[3,] 0.1286899 0.6102370 0.7158043 0.3710345 0.1742343
[4,] 0.1165784 0.7754142 0.6849693 0.3187890 0.1042490
[5,] 0.1357796 0.7315018 0.7167086 0.2849859 0.1310240
[6,] 0.1183157 0.7295458 0.7450450 0.2588463 0.1482472
```


### Function 5. The powering.

We check that $y\oplus y = 2\odot y$:

```{r}
head(as.matrix(2*comp_simData))
```

```{}
        SimS_A    SimS_B    SimS_C    SimS_D    SimS_E
[1,] 0.1385482 0.7440606 0.6338436 0.2998448 0.1837027
[2,] 0.1461806 0.6591945 0.7184241 0.3114824 0.1647184
[3,] 0.1286899 0.6102370 0.7158043 0.3710345 0.1742343
[4,] 0.1165784 0.7754142 0.6849693 0.3187890 0.1042490
[5,] 0.1357796 0.7315018 0.7167086 0.2849859 0.1310240
[6,] 0.1183157 0.7295458 0.7450450 0.2588463 0.1482472
```


